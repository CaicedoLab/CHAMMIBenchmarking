{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bcede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import skimage\n",
    "from importlib import reload\n",
    "import os\n",
    "import folded_dataset\n",
    "reload(folded_dataset)\n",
    "import utils\n",
    "reload(utils)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b069124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scr/zchen/datasets/morphem_70k_2.0'\n",
    "dataset = 'CP'\n",
    "model_choice = 'knn' # 'knn' or 'sgd'\n",
    "\n",
    "feature_filename = \"pretrained_resnet18_features.npy\"\n",
    "output_filename = \"resnet18_knn_sgd.csv\"\n",
    "\n",
    "leave_out = 'Task_four' # Leave-one-out task, set to None for Allen, 'Task_three' for HPA, 'Task_four' for CP\n",
    "leaveout_label = 'Plate' # Leave-one-out column name, 'cell_type' for HPA, 'Plate' for CP\n",
    "\n",
    "dest_dir = f'{root_dir}/results' # directory to save results\n",
    "save_csv = True # Set to True when running the last dataset to save result csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff46eab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load features and metadata\n",
    "print('Load features...')\n",
    "\n",
    "features_path = f'{root_dir}/features/{dataset}/{feature_filename}'\n",
    "df_path = f'{root_dir}/{dataset}/enriched_meta.csv'\n",
    "\n",
    "features = np.load(features_path)\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# Count number of tasks\n",
    "tasks = list(df['train_test_split'].unique())\n",
    "tasks.remove('Train')\n",
    "if leave_out != None:\n",
    "    leaveout_ind = tasks.index(leave_out)\n",
    "\n",
    "\n",
    "# Get index for training and each testing set\n",
    "train_indices = np.where(df['train_test_split'] == 'Train')[0]\n",
    "all_test_indices = [np.where(df[task])[0] for task in tasks]\n",
    "\n",
    "# Convert categorical labels to integers    \n",
    "target_value = list(df['Label'].unique())\n",
    "\n",
    "encoded_target = {}\n",
    "for i in range(len(target_value)):\n",
    "    encoded_target[target_value[i]] = i\n",
    "df['encoded_label'] = df.Label.apply(lambda x: encoded_target[x])\n",
    "\n",
    "# Split data into training and testing for regular classification\n",
    "train_X = features[train_indices]\n",
    "test_Xs = [features[test_indices] for test_indices in all_test_indices]\n",
    "\n",
    "task_Ys = [df['encoded_label'].values for key in tasks]\n",
    "train_Ys = [task_Ys[task_ind][train_indices] for task_ind in range(len(tasks))]\n",
    "test_Ys = [task_Ys[task_ind][test_indices] for task_ind, test_indices in enumerate(all_test_indices)]\n",
    "\n",
    "# Data splitting for leave one out task\n",
    "if leave_out != None:\n",
    "    df_takeout = df[df[leave_out]]\n",
    "    groups = list(df_takeout[leaveout_label].unique())\n",
    "\n",
    "    all_group_indices = [df_takeout[df_takeout[leaveout_label]==group].index.values for group in groups]\n",
    "    all_other_indices = [df_takeout[df_takeout[leaveout_label]!=group].index.values for group in groups]\n",
    "\n",
    "    takeout_X = [features[group_indices] for group_indices in all_group_indices]\n",
    "    rest_X = [features[np.concatenate((train_indices,other_indices), axis=None)] \\\n",
    "                                          for other_indices in all_other_indices]\n",
    "\n",
    "    takeout_Y = [task_Ys[leaveout_ind][group_indices] for group_indices in all_group_indices]\n",
    "    rest_Y = [task_Ys[leaveout_ind][np.concatenate((train_indices,other_indices), axis=None)] \\\n",
    "                                                  for other_indices in all_other_indices]\n",
    "\n",
    "print('Train classifiers...')\n",
    "accuracies = []\n",
    "f1scores_macro = []\n",
    "reports_str = []\n",
    "reports_dict = []\n",
    "\n",
    "\n",
    "\n",
    "for task_ind, task in enumerate(tasks):\n",
    "    if task != leave_out: # standard classification\n",
    "        \n",
    "        if model_choice == 'knn':\n",
    "            model = utils.FaissKNeighbors(k=1)\n",
    "        elif model_choice == 'sgd':\n",
    "            model = SGDClassifier(alpha=0.001, max_iter=100)\n",
    "        else:\n",
    "            print(f'{model_choice} is not implemented. Try sgd or knn.')\n",
    "            break\n",
    "        \n",
    "        model.fit(train_X, train_Ys[task_ind])\n",
    "        predictions = model.predict(test_Xs[task_ind])\n",
    "        ground_truth = test_Ys[task_ind]\n",
    "    \n",
    "    else: # leave-one-out\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        for group_ind, group in enumerate(groups):\n",
    "            model = utils.FaissKNeighbors(k=1)\n",
    "            \n",
    "            model.fit(rest_X[group_ind], rest_Y[group_ind])\n",
    "            group_predictions = model.predict(takeout_X[group_ind])\n",
    "            group_ground_truth = takeout_Y[group_ind]\n",
    "\n",
    "            predictions.append(group_predictions)\n",
    "            ground_truth.append(group_ground_truth)\n",
    "    \n",
    "        predictions = np.concatenate(predictions)\n",
    "        ground_truth = np.concatenate(ground_truth)\n",
    "        \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = np.mean(predictions == ground_truth)\n",
    "    report_str = classification_report(ground_truth, predictions)\n",
    "    report_dict = classification_report(ground_truth, predictions, output_dict=True)\n",
    "    f1score_macro = f1_score(ground_truth, predictions, average='macro')\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    f1scores_macro.append(f1score_macro)\n",
    "    reports_str.append(report_str)\n",
    "    reports_dict.append(report_dict)    \n",
    "\n",
    "print('Results:')\n",
    "for task_ind, task in enumerate(tasks):\n",
    "    print(f'Results for {dataset} {task} with {model_choice} :')\n",
    "    print(reports_str[task_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reports_dict = {}\n",
    "full_reports_dict['target_encoding'] = encoded_target\n",
    "for task_ind, task in enumerate(tasks):\n",
    "    full_reports_dict[task] = reports_dict[task_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(dest_dir+ '/'):\n",
    "    os.makedirs(dest_dir+ '/')\n",
    "    \n",
    "dict_path = f'{dest_dir}/{dataset}_{model_choice}_full_results.json'\n",
    "with open(dict_path, 'w') as f:\n",
    "    json.dump(full_reports_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6da95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'results' in locals():\n",
    "    results_temp = pd.DataFrame({'source': [dataset for i in range(len(tasks))],\\\n",
    "                        'task': tasks,'model': [model_choice for i in range(len(tasks))],\\\n",
    "                        'accuracy': accuracies,'f1_score_macro': f1scores_macro})\n",
    "    results = pd.concat([results, results_temp]).reset_index(drop=True)\n",
    "\n",
    "else: \n",
    "    results = pd.DataFrame({'source': [dataset for i in range(len(tasks))],\\\n",
    "                        'task': tasks,'model': [model_choice for i in range(len(tasks))],\\\n",
    "                        'accuracy': accuracies,'f1_score_macro': f1scores_macro})\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_csv:\n",
    "    results.to_csv(f'{dest_dir}/{output_filename}', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
