{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42bcede9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import io\n",
    "import skimage\n",
    "from importlib import reload\n",
    "import folded_dataset\n",
    "reload(folded_dataset)\n",
    "import utils\n",
    "reload(utils)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import f1_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b069124d",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir = '/scr/zchen/datasets/morphem_70k_2.0'\n",
    "dataset = 'CP'\n",
    "leave_out = 'Task_four' # Leave-one-out task, set to None for Allen\n",
    "leaveout_label = 'Plate'\n",
    "model_choice = 'knn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff46eab2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load features...\n",
      "Train classifiers...\n",
      "Results:\n",
      "Results for CP Task_one with knn :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.69      0.78      0.73      4313\n",
      "           1       0.45      0.71      0.55       393\n",
      "           2       0.55      0.64      0.60      2702\n",
      "           3       0.73      0.57      0.64      5657\n",
      "\n",
      "    accuracy                           0.66     13065\n",
      "   macro avg       0.61      0.68      0.63     13065\n",
      "weighted avg       0.67      0.66      0.66     13065\n",
      "\n",
      "Results for CP Task_two with knn :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.27      0.30      0.28      3778\n",
      "           1       0.17      0.47      0.25       369\n",
      "           2       0.18      0.32      0.23      2209\n",
      "           3       0.65      0.46      0.54     10039\n",
      "\n",
      "    accuracy                           0.41     16395\n",
      "   macro avg       0.32      0.39      0.33     16395\n",
      "weighted avg       0.49      0.41      0.43     16395\n",
      "\n",
      "Results for CP Task_four with knn :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       0.00      0.00      0.00         0\n",
      "           2       0.00      0.00      0.00         0\n",
      "           3       0.00      0.00      0.00         0\n",
      "           4       0.29      0.07      0.11      4738\n",
      "           5       0.24      0.05      0.08      4113\n",
      "           6       0.26      0.04      0.07      3499\n",
      "\n",
      "    accuracy                           0.05     12350\n",
      "   macro avg       0.11      0.02      0.04     12350\n",
      "weighted avg       0.26      0.05      0.09     12350\n",
      "\n",
      "Results for CP Task_three with knn :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.36      0.40      0.38      3470\n",
      "           1       0.29      0.10      0.15      1504\n",
      "           2       0.26      0.26      0.26      2581\n",
      "           3       0.25      0.31      0.28      2520\n",
      "\n",
      "    accuracy                           0.30     10075\n",
      "   macro avg       0.29      0.27      0.27     10075\n",
      "weighted avg       0.30      0.30      0.29     10075\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load features and metadata\n",
    "print('Load features...')\n",
    "\n",
    "features_path = f'{root_dir}/features/{dataset}/pretrained_resnet18_features.npy'\n",
    "df_path = f'{root_dir}/{dataset}/enriched_meta.csv'\n",
    "\n",
    "features = np.load(features_path)\n",
    "df = pd.read_csv(df_path)\n",
    "\n",
    "# Count number of tasks\n",
    "tasks = list(df['train_test_split'].unique())\n",
    "tasks.remove('Train')\n",
    "if leave_out != None:\n",
    "    leaveout_ind = tasks.index(leave_out)\n",
    "\n",
    "\n",
    "# Get index for training and each testing set\n",
    "train_indices = np.where(df['train_test_split'] == 'Train')[0]\n",
    "all_test_indices = [np.where(df[task])[0] for task in tasks]\n",
    "\n",
    "# Convert categorical labels to integers    \n",
    "target_value = list(df['Label'].unique())\n",
    "\n",
    "encoded_target = {}\n",
    "for i in range(len(target_value)):\n",
    "    encoded_target[target_value[i]] = i\n",
    "df['encoded_label'] = df.Label.apply(lambda x: encoded_target[x])\n",
    "\n",
    "# Split data into training and testing for regular classification\n",
    "train_X = features[train_indices]\n",
    "test_Xs = [features[test_indices] for test_indices in all_test_indices]\n",
    "\n",
    "task_Ys = [df['encoded_label'].values for key in tasks]\n",
    "train_Ys = [task_Ys[task_ind][train_indices] for task_ind in range(len(tasks))]\n",
    "test_Ys = [task_Ys[task_ind][test_indices] for task_ind, test_indices in enumerate(all_test_indices)]\n",
    "\n",
    "# Data splitting for leave one out task\n",
    "if leave_out != None:\n",
    "    df_takeout = df[df[leave_out]]\n",
    "    groups = list(df_takeout[leaveout_label].unique())\n",
    "\n",
    "    all_group_indices = [df_takeout[df_takeout[leaveout_label]==group].index.values for group in groups]\n",
    "    all_other_indices = [df_takeout[df_takeout[leaveout_label]!=group].index.values for group in groups]\n",
    "\n",
    "    takeout_X = [features[group_indices] for group_indices in all_group_indices]\n",
    "    rest_X = [features[np.concatenate((train_indices,other_indices), axis=None)] \\\n",
    "                                          for other_indices in all_other_indices]\n",
    "\n",
    "    takeout_Y = [task_Ys[leaveout_ind][group_indices] for group_indices in all_group_indices]\n",
    "    rest_Y = [task_Ys[leaveout_ind][np.concatenate((train_indices,other_indices), axis=None)] \\\n",
    "                                                  for other_indices in all_other_indices]\n",
    "\n",
    "print('Train classifiers...')\n",
    "accuracies = []\n",
    "f1scores_macro = []\n",
    "reports_str = []\n",
    "reports_dict = []\n",
    "\n",
    "\n",
    "\n",
    "for task_ind, task in enumerate(tasks):\n",
    "    if task != leave_out: # standard classification\n",
    "        \n",
    "        if model_choice == 'knn':\n",
    "            model = utils.FaissKNeighbors(k=1)\n",
    "        elif model_choice == 'sgd':\n",
    "            model = SGDClassifier(alpha=0.001, max_iter=100)\n",
    "        else:\n",
    "            print(f'{model_choice} is not implemented. Try sgd or knn.')\n",
    "            break\n",
    "        \n",
    "        model.fit(train_X, train_Ys[task_ind])\n",
    "        predictions = model.predict(test_Xs[task_ind])\n",
    "        ground_truth = test_Ys[task_ind]\n",
    "    \n",
    "    else: # leave-one-out\n",
    "        predictions = []\n",
    "        ground_truth = []\n",
    "        for group_ind, group in enumerate(groups):\n",
    "            model = utils.FaissKNeighbors(k=1)\n",
    "            \n",
    "            model.fit(rest_X[group_ind], rest_Y[group_ind])\n",
    "            group_predictions = model.predict(takeout_X[group_ind])\n",
    "            group_ground_truth = takeout_Y[group_ind]\n",
    "\n",
    "            predictions.append(group_predictions)\n",
    "            ground_truth.append(group_ground_truth)\n",
    "    \n",
    "        predictions = np.concatenate(predictions)\n",
    "        ground_truth = np.concatenate(ground_truth)\n",
    "        \n",
    "    # Compute evaluation metrics\n",
    "    accuracy = np.mean(predictions == ground_truth)\n",
    "    report_str = classification_report(ground_truth, predictions)\n",
    "    report_dict = classification_report(ground_truth, predictions, output_dict=True)\n",
    "    f1score_macro = f1_score(ground_truth, predictions, average='macro')\n",
    "\n",
    "    accuracies.append(accuracy)\n",
    "    f1scores_macro.append(f1score_macro)\n",
    "    reports_str.append(report_str)\n",
    "    reports_dict.append(report_dict)    \n",
    "\n",
    "print('Results:')\n",
    "for task_ind, task in enumerate(tasks):\n",
    "    print(f'Results for {dataset} {task} with {model_choice} :')\n",
    "    print(reports_str[task_ind])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1b67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_reports_dict = {}\n",
    "full_reports_dict['target_encoding'] = encoded_target\n",
    "for task_ind, task in enumerate(tasks):\n",
    "    full_reports_dict[task] = reports_dict[task_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185bfea",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = '/scr/zchen/MorphEm_local/results'\n",
    "\n",
    "dict_path = f'{dest_dir}/{dataset}_{model_choice}_full_results.json'\n",
    "with open(dict_path, 'w') as f:\n",
    "    json.dump(full_reports_dict, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b6da95e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>task</th>\n",
       "      <th>model</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>f1_score_macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>HPA</td>\n",
       "      <td>Task_one</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.520650</td>\n",
       "      <td>0.522278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HPA</td>\n",
       "      <td>Task_two</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.378378</td>\n",
       "      <td>0.334066</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HPA</td>\n",
       "      <td>Task_three</td>\n",
       "      <td>knn</td>\n",
       "      <td>0.142424</td>\n",
       "      <td>0.089350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source        task model  accuracy  f1_score_macro\n",
       "0    HPA    Task_one   knn  0.520650        0.522278\n",
       "1    HPA    Task_two   knn  0.378378        0.334066\n",
       "2    HPA  Task_three   knn  0.142424        0.089350"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_temp = pd.DataFrame({'source': [dataset for i in range(len(tasks))],\\\n",
    "                        'task': tasks,'model': [model_choice for i in range(len(tasks))],\\\n",
    "                        'accuracy': accuracies,'f1_score_macro': f1scores_macro})\n",
    "results_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c11165c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results, results_temp]).reset_index(drop=True)\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb01a07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dest_dir = '/scr/zchen/MorphEm_local/results'\n",
    "results.to_csv(f'{dest_dir}/resnet18_knn_sgd.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
